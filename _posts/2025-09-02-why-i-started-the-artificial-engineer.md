---
layout: single
title: "Why I Started The Artificial Engineer"
permalink: /why-this-blog/
date: 2025-09-02
categories: [everyone]
tags: [ml-engineering, reproducibility, learning-in-public]
excerpt: "A two-track ML blog: reproducible engineering deep dives for practitioners, and clear explainers for everyone."
header:
  teaser: /assets/images/artificial-engineer-teaser.png
---
## The Problem
After my 23rd attempt at debugging an inference pipeline at 2 AM on a Sunday I realized most ML tutorials end exactly where real problems begin.


## My Path Here
I didn‚Äôt come from a traditional CS background‚ÄîI studied **economics and law**. But curiosity (and a lot of debugging in PyTorch) led me into machine learning.  

Over the last 4+ years, I‚Äôve worked on:
- Evaluation frameworks such as [Dynabench](https://dynabench.org) 
- Large-scale datasets such as [People's Speech](https://huggingface.co/datasets/MLCommons/peoples_speech) and [Adversarial Nibbler](https://github.com/google-research-datasets/adversarial-nibbler/tree/main)
- Publishing at top conferences
  - Best paper award at NeurIPS D&B 2024 with [The Prism Alignment Dataset](https://neurips.cc/virtual/2024/oral/98025)
  - [BabyLM](https://arxiv.org/abs/2504.08165) at CONNL (ACL) 2023
  - [DataPerf](https://proceedings.neurips.cc/paper_files/paper/2023/hash/112db88215e25b3ae2750e9eefcded94-Abstract-Datasets_and_Benchmarks.html) at NeurIPS 2023
- Some competitions every now and then such as [PatentBot](https://devpost.com/software/patentbot?_gl=1*16py2bb*_gcl_au*NTAxNzYwNjUzLjE3NTY4NjI5MzI.*_ga*MTA1NjczNzEzNi4xNzU2ODYyOTMz*_ga_0YHJK3Y10M*czE3NTY4NjI5MzIkbzEkZzEkdDE3NTY4NjI5MzUkajU3JGwwJGgw)

Through this work, I noticed a consistent pattern: the gap between research papers and production reality is bigger than anyone admits.

- **Technical content**: often too dense for anyone outside the research bubble  
- **Popular ML content**: often oversimplified to the point of being misleading  
- **Bridges between the two**: almost nonexistent  

That‚Äôs the problem this blog tries to solve.

## Two Tracks, One Mission

This blog runs on two parallel tracks‚Äîeach aimed at closing the gap between **research depth** and **real-world clarity**.

### üîß For Engineers
Deep technical dives, reproducible experiments, and hard-won lessons from production. Sample posts:
- *Why our LangChain pipeline breaks when trying to customize it*  
- *Scaling inference when GPUs are scarce (and expensive)*  
- *Fine-tuning small LLMs for common use cases ‚Äî a cheaper path than you think*  
- *Building a production-ready ASR pipeline: from VAD to deployment*  

### üåç For Everyone
Clear, accessible explainers of how ML shapes daily life ‚Äî no PhD required, no jargon walls. Sample posts:
- *Why LLMs hallucinate (and why it matters in practice)*  
- *Getting more out of your AI assistant without burning tokens*  
- *Tips for better image generations with today‚Äôs generative models*  
- *How recommendation systems really guess your taste*  
 
## Why ‚ÄúThe Artificial Engineer‚Äù?

I'm not a "real" engineer by degree, but I've spent four years making AI systems work in the real world. The name captures the irony: someone artificial (by training) engineering artificial intelligence.

## Join the Conversation

I‚Äôll be posting **twice a month** (one per track). But I want this to be a dialogue, not a broadcast.    

Every ML engineer has that moment when the tutorial ends and reality begins. I'm documenting everything that happens next‚Äîthe debugging, the scaling, the "why didn't anyone mention this?" moments. Subscribe below to follow along.

